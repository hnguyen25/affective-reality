# Affective Reality

## About
Affective reality is a [Stanford BCI](https://stanfordbci.com) project which seeks to use biometric data to recognize one's emotional and focused state, and then use this to inform a virtual reality environment--whether that be a meditation app whose environment and soundscape changes depending on mood and relaxation, or an education app that monitors focus to increase learning outcomes. The end goal is to create a closed loop, bi-directional system wherein one's mental states influences the VR environment, and conversely, the VR environment influences one's mental state.

## Documentation
To read more about this codebase, visit this link:

## How to run
The project consists of different subsections:
- (A) Machine learning models which classifies EEG data into various emotional/affective metrics
- (B) A VR application made with Unity, which is used to collect experimental EEG data for emotional classification
- (C) A VR application made with Unity, which does real-time, emotion classification based on (A) and (B)

### Running emotion classification models
In order to run part (A) of the project:

### Running emotion elicitation experiments
In order to run part (B) of the project:

### Running real-time emotion classification applications
In order to run part (C) of the project:

## Results

### Performance of classification models on publicly available datasets

### Performance of classification models on self-collected data


